{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH540yEwYgLM"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 torchaudio==0.10.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjHnmUM2lzZe"
      },
      "outputs": [],
      "source": [
        " # Install required packages.\n",
        "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install -q pytorch-lightning\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLZWm9HChwWN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F \n",
        "import torch_geometric.transforms as T\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "from torch.optim import lr_scheduler\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "import torch_geometric.nn\n",
        "from torch_geometric.nn import GraphConv, TopKPooling, SplineConv\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eps3t4gGbEZ"
      },
      "outputs": [],
      "source": [
        "from math import sin, cos\n",
        "import copy\n",
        "import random\n",
        "\n",
        "def rotation(graph):\n",
        "  theta = np.random.uniform(-Theta, Theta)\n",
        "  a = np.array([[cos(theta), -sin(theta)],\n",
        "                [sin(theta), cos(theta)]])\n",
        "  \n",
        "  g = graph.edge_attr[:,:2]\n",
        "  p = np.apply_along_axis(np.dot, 1, g, a)\n",
        "  graph.edge_attr[:,:2] = torch.from_numpy(p[:,:2])\n",
        "  return graph\n",
        "\n",
        "def scaling(graph):\n",
        "  s = np.random.uniform(1/S, S, size=2)\n",
        "  a = np.array([[s[0], 0],\n",
        "                [0, s[1]]])\n",
        "  \n",
        "  g = graph.edge_attr[:,:2]\n",
        "  p = np.apply_along_axis(np.dot, 1, g, a)\n",
        "  graph.edge_attr[:,:2] = torch.from_numpy(p[:,:2])\n",
        "  return graph\n",
        "\n",
        "def translation(p):\n",
        "  if p[0]==0 and p[1]==0:\n",
        "    return np.asarray(p)\n",
        "  else:\n",
        "    return np.asarray(p) + np.random.uniform(-TT, TT, size=2)\n",
        "\n",
        "# TT = 0.1;\n",
        "# Theta = 0.6;\n",
        "# S = 1.4;\n",
        "\n",
        "TT = 0;\n",
        "Theta = 0;\n",
        "S = 1;\n",
        "\n",
        "class my_affine_transforms(T.BaseTransform):\n",
        "  def __init__(self, args = []):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, data: torch_geometric.data.data.Data):\n",
        "    data = rotation(data)\n",
        "    data = scaling(data)\n",
        "\n",
        "    data.edge_attr[:, :2] = torch.from_numpy(np.apply_along_axis(translation, 1, data.edge_attr[:, :2]))\n",
        "\n",
        "    # data.edge_attr[:, :2] = torch.from_numpy(np.apply_along_axis(trnsfrms, 1, data.edge_attr[:, :2]))\n",
        "    \n",
        "    # for p in data.edge_attr:\n",
        "    #   if p[0] != 0 or p[1] != 0:\n",
        "    #     p[:2] = torch.from_numpy(rotation(p[:2]))\n",
        "    #     p[:2] = torch.from_numpy(scaling(p[:2]))\n",
        "    #     p[:2] = torch.from_numpy(translation(p[:2]))\n",
        "    return data\n",
        "  \n",
        "  def __repr__(self) -> str:\n",
        "    return (f'{self.__class__.__name__}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCrvCm1QwPH5"
      },
      "outputs": [],
      "source": [
        "class my_normalization(T.BaseTransform):\n",
        "  def __init__(self, args = []):\n",
        "    pass\n",
        "\n",
        "  def my_norm(x, a, b):\n",
        "    return (x-a)/(b-a)\n",
        "\n",
        "  def __call__(self, data: torch_geometric.data.data.Data):\n",
        "    my_norm_vect = np.vectorize(my_normalization.my_norm)\n",
        "    x = np.array(data.edge_attr)\n",
        "    x[:,0] = my_norm_vect(x[:,0],  min(x[:,0]), max(x[:,0]))\n",
        "    x[:,1] = my_norm_vect(x[:,1],  min(x[:,1]), max(x[:,1]))\n",
        "    data.edge_attr[:, 0] = torch.from_numpy(x[:, 0])\n",
        "    data.edge_attr[:, 1] = torch.from_numpy(x[:, 1])\n",
        "    return data\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "    return (f'{self.__class__.__name__},')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9axYBYwvWDhG"
      },
      "outputs": [],
      "source": [
        "dataset = TUDataset(root='data/TUDataset', name='Cuneiform', transform=T.Compose([my_normalization()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pum7xm-n4unn"
      },
      "outputs": [],
      "source": [
        "class Net(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # kernel_size = 5\n",
        "        kernel_size = 5\n",
        "\n",
        "        dim=dataset.num_edge_features\n",
        "        degree = 1\n",
        "\n",
        "        self.conv1 = SplineConv(dataset.num_features, 32, dim=dim, kernel_size=kernel_size, degree=degree)\n",
        "        self.conv2 = SplineConv(32, 64, dim=dim, kernel_size=kernel_size, degree=degree)\n",
        "        self.conv3 = SplineConv(64, 64, dim=dim, kernel_size=kernel_size, degree=degree)\n",
        "        \n",
        "        self.lin1 = torch.nn.Linear(64, dataset.num_classes)\n",
        "\n",
        "        self.dropout = 0.2\n",
        "\n",
        "        self.train_accuracy = torchmetrics.Accuracy()\n",
        "        self.val_accuracy = torchmetrics.Accuracy()\n",
        "        self.test_accuracy = torchmetrics.Accuracy()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pseudo = data.edge_attr\n",
        "\n",
        "        in_dropout = 0.2\n",
        "        # in_dropout = 0.5\n",
        "        # in_dropout = 0.3\n",
        "\n",
        "        x = F.elu(self.conv1(x, edge_index, pseudo))\n",
        "        x = F.dropout(x, in_dropout, training = self.training)\n",
        "        x = F.elu(self.conv2(x, edge_index, pseudo))\n",
        "        x = F.dropout(x, in_dropout, training = self.training)\n",
        "        x = F.elu(self.conv3(x, edge_index, pseudo))\n",
        "        # x = F.dropout(x, in_dropout, training = self.training)\n",
        "        \n",
        "        # x = F.elu(self.conv4(x, edge_index, pseudo))\n",
        "        # x = F.dropout(x, self.dropout, training = self.training)\n",
        "\n",
        "        x = gap(x, batch)\n",
        "\n",
        "        x = F.dropout(x, self.dropout, training = self.training)\n",
        "\n",
        "        x = F.log_softmax(self.lin1(x), dim=1)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      output = self(batch)\n",
        "      # loss = F.nll_loss(output, batch.y)\n",
        "      loss = F.cross_entropy(output, batch.y)\n",
        "      self.log('training_loss', loss, on_epoch=True, on_step=False)\n",
        "      self.log('train_acc_step', self.train_accuracy(output, batch.y))\n",
        "      return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "      self.log('train_acc_epoch', self.train_accuracy.compute())\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
        "      # \"an initial learning rate of 0.01 and learning rate decay to 0.001 after 200 epochs\"\n",
        "      scheduler = lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[200], gamma = 0.1)\n",
        "      return {\n",
        "        \"optimizer\": optimizer,\n",
        "        \"lr_scheduler\": scheduler\n",
        "      }\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "      output = self(batch)\n",
        "      # loss = F.nll_loss(output, batch.y)\n",
        "      loss = F.cross_entropy(output, batch.y)\n",
        "      self.log('val_loss', loss, on_epoch=True, on_step=False)\n",
        "      self.log('val_acc_step', self.val_accuracy(output, batch.y))\n",
        "      return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "      self.log('val_acc_epoch', self.val_accuracy.compute())\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "      output = self(batch)\n",
        "      # loss = F.nll_loss(output, batch.y)\n",
        "      loss = F.cross_entropy(output, batch.y)\n",
        "      self.log('test_loss', loss, on_epoch=True, on_step=False)\n",
        "      self.log('test_acc_step', self.test_accuracy(output, batch.y))\n",
        "      return loss\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "      self.log('test_acc_epoch', self.test_accuracy.compute())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_ufXHm0cAdf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNWnGrab-wGO"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TdpS2pA3wyU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datetime import datetime\n",
        "p = '/content/drive/My Drive/results/' + str(datetime.now())+'.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8ean4hk2OPa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-znT8dAi4Qa"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/checkpoints/\n",
        "k_folds = 10\n",
        "max_epochs = 300\n",
        "# 20000\n",
        "# 300\n",
        "results = []\n",
        "tstamps = []\n",
        "batch_size = 64\n",
        "\n",
        "TT = 0.1;\n",
        "Theta = 0.06;\n",
        "S = 2;  \n",
        "\n",
        "# TT = 0.1;\n",
        "# Theta = 0.6;\n",
        "# S = 1.04;\n",
        "\n",
        "\n",
        "early_stopping = False\n",
        "np_random = True\n",
        "kfold_random = True\n",
        "\n",
        "# kfold = KFold(n_splits=k_folds, shuffle=kfold_random, random_state=0)\n",
        "kfold = KFold(n_splits=k_folds, shuffle=kfold_random)\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='Cuneiform', transform=T.Compose([my_normalization()]))\n",
        "augmentation_dataset = TUDataset(root='data/TUDataset', name='Cuneiform', transform=T.Compose([my_affine_transforms(), my_normalization()]))\n",
        "\n",
        "# np.random.seed(12345)\n",
        "model = None\n",
        "\n",
        "for i in range(1,11):\n",
        "  results.append({})\n",
        "  tstamps.append([])\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "  # if fold != 0:\n",
        "  #   continue\n",
        "  # Print\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "\n",
        "  tstamps[fold].append(datetime.now())\n",
        "\n",
        "  del model \n",
        "  model = Net()\n",
        "\n",
        "  if early_stopping:\n",
        "    if np_random:\n",
        "      np.random.shuffle(train_ids)\n",
        "    \n",
        "    val_size = len(train_ids) // 15\n",
        "    val_ids = list(train_ids[:val_size])\n",
        "    train_ids = list(train_ids[val_size:])\n",
        "    val_loader = DataLoader(dataset[val_ids], batch_size=batch_size)\n",
        "\n",
        "    dataset = TUDataset(root='data/TUDataset', name='Cuneiform', transform=T.Compose([my_normalization()]))\n",
        "    # augmentation_dataset = TUDataset(root='data/TUDataset', name='Cuneiform', transform=T.Compose([my_affine_transforms(), my_normalization()]))\n",
        "\n",
        "    test_loader = DataLoader(dataset[test_ids], batch_size=batch_size)\n",
        "    # val_loader = test_loader # DataLoader(dataset[test_ids], batch_size=batch_size)\n",
        "    train_loader = DataLoader(dataset[train_ids], batch_size=batch_size)\n",
        "    augmented_train_loader = DataLoader(augmentation_dataset[train_ids], batch_size=batch_size)\n",
        "\n",
        "    # model.apply(reset_weights)    \n",
        "    early_stop_callback = EarlyStopping(monitor=\"val_acc_epoch\", \n",
        "                                        min_delta=0.00, \n",
        "                                        patience=1000, \n",
        "                                        verbose=False, \n",
        "                                        mode=\"max\")\n",
        "    checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints\",\n",
        "                                          filename=\"best-checkpoint-fold-\"+str(fold),\n",
        "                                          save_top_k=1,\n",
        "                                          verbose=False,\n",
        "                                          monitor=\"val_acc_epoch\",\n",
        "                                          mode=\"max\",\n",
        "                                          every_n_epochs = 1,\n",
        "                                          save_last=True)\n",
        "\n",
        "    logger = TensorBoardLogger(\"lightning_logs\", name=\"model\", default_hp_metric= False)\n",
        "    trainer = pl.Trainer(gpus=1, # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "                        logger=logger,\n",
        "                        callbacks=[early_stop_callback, checkpoint_callback], \n",
        "                        # callbacks=[checkpoint_callback], \n",
        "                        check_val_every_n_epoch=1,\n",
        "                        max_epochs=max_epochs)\n",
        "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "    \n",
        "    checkpoint = torch.load(\"/content/checkpoints/best-checkpoint-fold-\"+str(fold)+\".ckpt\")\n",
        "    model.load_state_dict(checkpoint['state_dict'])  \n",
        "  else:  \n",
        "    if np_random:\n",
        "      np.random.shuffle(train_ids)\n",
        "\n",
        "    test_loader = DataLoader(dataset[test_ids], batch_size=batch_size)\n",
        "    train_loader = DataLoader(dataset[train_ids], batch_size=batch_size)\n",
        "    augmented_train_loader = DataLoader(augmentation_dataset, batch_size=batch_size)\n",
        "\n",
        "    model.apply(reset_weights)\n",
        "\n",
        "    logger = TensorBoardLogger(\"lightning_logs\", name=\"model\", default_hp_metric= False)\n",
        "    trainer = pl.Trainer(gpus=1,# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!11 \n",
        "                         logger=logger, \n",
        "                         check_val_every_n_epoch=1, \n",
        "                         max_epochs=max_epochs,\n",
        "                         replace_sampler_ddp=True,\n",
        "                         )\n",
        "\n",
        "    trainer.fit(model=model, train_dataloaders=train_loader)\n",
        "\n",
        "    # trainer.fit(model=model, train_dataloader=augmented_train_loader)\n",
        "  # ---------------------- #  \n",
        "\n",
        "  # Завантажує кращу модель у фолді, і проводить тест на ній.\n",
        "    \n",
        "  tstamps[fold].append(datetime.now())\n",
        "  # results.append({})\n",
        "  results[fold]['epoch'] = model.current_epoch\n",
        "  results[fold]['accuracy'] = trainer.test(model=model, dataloaders=test_loader)\n",
        "  print(results[fold])\n",
        "  with open(p, 'w') as f:\n",
        "    f.write(str(results))\n",
        "    f.write(\"\\n\\n\")\n",
        "    f.write(str(tstamps))\n",
        "  # if fold == 0:\n",
        "  #   break;\n",
        "\n",
        "!cp -r /content/checkpoints/ /content/drive/MyDrive/Masters/checkpoints+$(date +\"%Y-%m-%d-%T\")\n",
        "!cp -r lightning_logs/model/ /content/drive/MyDrive/Masters/lightning_logs+$(date +\"%Y-%m-%d-%T\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Swtac2UqGA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w08WSFSVWfeh"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Code Listing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}